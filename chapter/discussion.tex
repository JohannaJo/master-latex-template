\chapter{Discussion}

- Other rule mining algorithms, Rudik OP.

The \textit{area under the precision-recall curve} (AUC-PR) is a metric appropriate for evaluating the performance of KG embedding models \cite{jahn2021reasoning}. A precision-recall curve is a plot of the precision over the y-axis and recall over the x-axis. AUC-PR is thus the area under this curve and describes how good a model is at predicting the positive class. This metric could have been used in the thesis for evaluation of the KG embedding models, but was omitted as it requires context for interpretation. A model that assigns a random score to a triple will result in a horizontal line based on the distribution of classified triples (positive or negative). Therefore a AUC-PR score of 0.5 is good if 99\% of the classified triples are negative, but if the triples are evenly distributed then a AUC-PC score of 0.5 would indicate that the model classified no better than randomly.


\section{Mining ontologies through queries}
This approach is similar to the KG extension and mining pipleline used for experiments in this thesis, but differs in the manner in which the KG embeddings are used to add implicit information to the KG. Instead of a rule mining algorithm that takes a KG as input, this approach uses the HORN algorithm by Angluin et al \cite{DBLP:journals/ml/AngluinFP92}. which requires an ``oracle" to learn from. The oracle can be viewed as a teacher that is considered to be an expert on the rules we would like to learn, with the idea being to use a well-trained KG embedding model as the oracle for the HORN algorithm. The HORN algorithm outputs rules in the form of propositional logic. If the reader is unfamiliar with propositional logic, a brief introduction covering the relevant aspects can be found in the appendix at section \ref{propositional_logic}. Now we will examine the HORN algorithm, before explaining why it was incompatible with RDF-style KGs.

\subsection{The HORN algorithm}
Similarly to AMIE3, the HORN algorithm is designed to output a set of rules in the form of a Horn sentence. It does this by posing equivalence queries and membership queries. An equivalence query asks if the current hypothesis, a Horn sentence, is equivalent with the target Horn sentence that the oracle knows. If the oracle answers ``yes", then the target set of rules has been found and the algorithm terminates. Otherwise, a ``no" answer is accompanied with a valuation in which the target and hypothesis are evaluated to different boolean values. A \textit{positive} valuation is one which makes the target Horn sentence true, while a \textit{negative} valuation is one which evaluated the target to false. %If we consider $Q$ as the target Horn sentence, then the valuation $\mathcal{V}_{+}$ is a positive example of $Q$ and $\mathcal{V}_{-}$ is a negative example of $Q$.

A membership query asks whether a given valuation is positive or negative, to which the oracle only answers ``yes" or ``no". We will now focus on the implementation of the oracle and why this was incompatible with the set-of-triples data format. For more information on the HORN algorithm, please refer to the work \textit{Learning conjunctions of Horn clauses} by Angluin et al. \cite{DBLP:journals/ml/AngluinFP92}. The important takeaway for this algorithm is that it runs in polynomial time on the size of the target and the number of variables considered.


\subsection{Incompatibility of KGs as triples and PL}
If the oracle were a machine learning model, it would need to be able to classify valuations as positive or negative. The equivalence query could be simulated with many membership queries, where if enough valuations are tested one can with high probability determine whether the target is logically equivalent with the hypothesis. So all we really need to simulate the oracle is a binary classifier. This approach rested on the idea that with enough valuations labeled as positive/negative, one could train a model to implicitly represent the target Horn sentence that determines if the valuations are positive or negative. 

The problem with applying this idea to KGs arises immediately when trying to express all entities and relations of a KG with literals. This is necessary in order to create a set of positive valuations that collectively represent the all information in the KG. The approach of assigning an individual literal to each entity and relation is not feasible. YAGO4 \cite{yago4}, for example, contains over 50 million distinct entities, meaning that each data point in the training set would have at least 50 million features. One can attempt to assign shared \textit{roles} to entities to limit the number of elements to encode, but the erasure of the unique identity of entities leads to further problems when it comes to maintaining patters in the data. To demonstrate this let us again use the KG from example \ref{mini_KG_rules}. Assume we have some intelligent way of assigning roles to all the individuals in the KG, which in this simple example will be the role of ``adult" and ``child". \texttt{Carol} will be an adult, and \texttt{Ann} and \texttt{Bob} children. Now we have reduced the number of entities by one, and the KG now looks like this:

\begin{example}[An even simpler KG.]
\begin{lstlisting}[]
<child> <hasParent> <adult>
<adult> <hasChild> <child>
<adult> <hasChild> <child>
<child> <hasSibling> <child>
<child> <hasSibling> <child>
\end{lstlisting}
\label{mini_simple_KG_rules}
\end{example}

Now the KG has been simplified and almost half the data points have become redundant. If we now consider the rule \[hasSibling(x, y) \wedge hasParent(y,z) \Rightarrow hasParent(x,z)\] which previously predicted the new triple \texttt{(Bob, hasParent, Carol)}, it now only predicts \texttt{(child, hasParent, adult)} which is not new information. By removing the identity of the individuals in the family, we removed the information that Ann is specifically the sibling of Bob and has Carol as a parent, which implied that Bob \textit{also} had Carol as a parent. Hence, the valuations must represent all unique entities, or find a way to create replacement labels that maintain all the relevant information about the roles entities have in the KG. This becomes unfeasible with large KGs, such as the mentioned YAGO4.

\section{Further work}


There are a great number of ways to expand upon the work in this thesis. As mentioned in section \ref{experiment_limitations} the main limitations of the experiment itself were due to computational restrictions. The quality of the results is likely to substantially improve if larger and more interesting KGs were used, as when KGs are restricted to only six relation types there is little area to create rules within. The \textit{form} of the rules was also limited. Rules contained no entities, only variables, therefore relevant rules such as
\[wonAward(x, Spellemannprisen) \Rightarrow citizenOf(x, Norway)\]
were never considered. Section \ref{the_amies} explained how AMIE3 \textit{is} capable of mining such rules with instantiated variables, but at the expense of increased runtime, which is the reason why it was not done the experiments.

Rules mined from a KG can indicate new knowledge, but if the data they are extracted from contain errors and/or biases, then it may lead to erronous rules. For example, in the YAGO3\_10 dataset 99\% of the football players in the data are men, which would lead to the erronous rule that "\textit{If $x$ is a football player, then $x$ is a man.}"

How much noise needs to be added for a rule to not be mined anymore. Intuitively: the more noise the less patterns.

The output for the pipeline in the experiments of this thesis is a set of rules, which can have many different applications, for example ontology generation or explainable AI. If the application on the other hand is to use the rules to make new predictions, then an inverse approach can be used to achieve this. Instead of using KG embedding models to extend the KG to mine better/more rules from, to them make predictions, one can use a set of mined rules to extend the KG, possibly resulting in a better KG embedding model. Work by Meilicke et al. \cite{ensemble} showed that an ensemble of rule-based and embedding methods made the best predictions, as they complemented each other's shortcomings. This meant that multiple classifiers ``voted" on triple prediction. In their experiments their ensemble consisted of their own simple rule mining algorithm RuleN and AMIE for the rule-based approach, and TransE, HolE and RESCAL from the embedding-based approach.