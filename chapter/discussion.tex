\chapter{Discussion}
Prior to the development of the idea explored in this thesis, a different proposal for extracting rules from \glspl{kg} was considered. It has similarities with the current work, but ultimately was deemed unachievable to implement. This chapter starts by explaining this failed idea and explain why it was unsuccessful, before summarising the findings of the more successful work described this thesis and provide an overall evaluation. Certain design choices are assessed and the chapter concludes with propositions for further work.

\section{Mining ontologies through queries}
This abandoned approach is similar to the \gls{kg} extension and mining pipleline used for experiments in this study, but differs in the manner in which the \glspl{kge} are used to add implicit information to the \gls{kg}. Instead of a rule mining algorithm that takes a \gls{kg} as input, this approach uses the HORN algorithm by Angluin et al \cite{DBLP:journals/ml/AngluinFP92}, which requires an ``oracle" to learn from. The oracle can be viewed as a teacher that is considered to be an expert on the rules we would like to learn, with the idea being to use a well-trained \gls{kge} model as the oracle for the HORN algorithm. The HORN algorithm outputs rules in the form of propositional logic. If the reader is unfamiliar with propositional logic, a brief introduction covering the relevant aspects can be found in the appendix at section \ref{propositional_logic}. Below, we will examine the HORN algorithm, before explaining why it was incompatible with RDF-style \glspl{kg}.

\subsection{The HORN algorithm}
Similarly to AMIE3, the HORN algorithm is designed to output a set of rules in the form of a Horn sentence. It does this by posing equivalence queries and membership queries. An equivalence query asks if the current hypothesis, a Horn sentence, is equivalent with the target Horn sentence that the oracle knows. If the oracle answers ``yes", then the target set of rules has been found and the algorithm terminates. Otherwise, a ``no" answer is accompanied with a valuation in which the target and hypothesis are evaluated to different boolean values. A \textit{positive} valuation is one which makes the target Horn sentence true, while a \textit{negative} valuation is one which evaluated the target to false. %If we consider $Q$ as the target Horn sentence, then the valuation $\mathcal{V}_{+}$ is a positive example of $Q$ and $\mathcal{V}_{-}$ is a negative example of $Q$.

A membership query asks whether a given valuation is positive or negative, to which the oracle only answers ``yes" or ``no". We will now focus on the implementation of the oracle and why this was incompatible with the set-of-triples data format. For more information on the HORN algorithm, please refer to the work \textit{Learning conjunctions of Horn clauses} by Angluin et al. \cite{DBLP:journals/ml/AngluinFP92}. The important takeaway for this algorithm is that it runs in polynomial time on the size of the target and the number of variables considered.


\subsection{Incompatibility of KGs as triples and PL}
If the oracle were a machine learning model, it would need to be able to classify valuations as positive or negative. The equivalence query could be simulated with many membership queries, where if enough valuations are tested one can with high probability determine whether the target is logically equivalent with the hypothesis. So all we really need to simulate the oracle is a binary classifier. This approach rested on the idea that with enough valuations labeled as positive/negative, one could train a model to implicitly represent the target Horn sentence that determines if the valuations are positive or negative. 

The problem with applying this idea to \glspl{kg} arises immediately when trying to express all entities and relations of a \gls{kg} with literals. This is necessary in order to create a set of positive valuations that collectively represent all information in the \gls{kg}. The approach of assigning an individual literal to each entity and relation is not feasible. YAGO4 \cite{yago4}, for example, contains over 50 million distinct entities, meaning that each data point in the training set would have at least 50 million features. One can attempt to assign shared \textit{roles} to entities to limit the number of elements to encode, but the erasure of the unique identity of entities leads to further problems when it comes to maintaining patters in the data. To demonstrate this let us again use the \gls{kg} from example \ref{mini_KG_rules}. Assume we have some intelligent way of assigning roles to all the individuals in the \gls{kg}, which in this simple example will be the role of ``adult" and ``child". \texttt{Carol} will be an adult, and \texttt{Ann} and \texttt{Bob} children. Now we have reduced the number of entities by one, and the \gls{kg} now looks like this:

\begin{example}[An even simpler KG. TODO: fix numbering.]
\begin{lstlisting}[]
<child> <hasParent> <adult>
<adult> <hasChild> <child>
<adult> <hasChild> <child>
<child> <hasSibling> <child>
<child> <hasSibling> <child>
\end{lstlisting}
\label{mini_simple_KG_rules}
\end{example}

Now the \gls{kg} has been simplified and almost half the data points have become redundant. If we now consider the rule \[hasSibling(x, y) \wedge hasParent(y,z) \Rightarrow hasParent(x,z)\] which previously predicted the new triple \texttt{(Bob, hasParent, Carol)}, it now only predicts \texttt{(child, hasParent, adult)} which is not new information. By removing the identity of the individuals in the family, we removed the information that Ann is specifically the sibling of Bob and has Carol as a parent, which implied that Bob \textit{also} has Carol as a parent. Hence, the valuations must represent all unique entities, or find a way to create replacement labels that maintain all the relevant information about the roles entities have in the \gls{kg}. This becomes unfeasible with large \glspl{kg}, such as the mentioned YAGO4.


\section{Conclusion of findings}
In this study we examined how adding new facts deemed plausible by a KGE affected the rules mined from the extended \gls{kg}. We showed that extending \glspl{kg} in such a way did in fact lead to rules being mined and that the quality of these rules (approximated by PCA confidence) was similar to that of the original rules, but only when the confidence score is calculated over the extended \gls{kg} from which the new rule is mined. It was also shown that there was a strong corrrelation between the PCA confidence of an original rule and whether it was re-mined after the \gls{kg} was extended.

When evaluating the effect of the three parameters in the experiment, the choice of KGE was shown to clearly be the one with the highest impact. The RandomBaseline led to no new rules being mined and the omittance of original rules with the lowest PCA confidence (calculated over the original KG). As the RandomBaseline was the only KGE that led to no new rules, we can conclude that the other embeddings found non-trivial patters in the data, reflected by the new rules mined on the extended dataset. \glspl{kg} that were extended with TransE led to an unusually large number of new rules, as the triples ranked highly by TransE contributed greatly to introducing new patterns in the \gls{kg}. There was surprisingly little overlap between the new rules mined from \glspl{kg} extended with different KGEs, underlining the impact of the choice of KGE.

\section{Discussion of design choices}
The \textit{area under the precision-recall curve} (AUC-PR) is a metric appropriate for evaluating the performance of \gls{kge} models \cite{jahn2021reasoning}. A precision-recall curve is a plot of the precision over the y-axis and recall over the x-axis. AUC-PR is thus the area under this curve and describes how well a model is at able to correctly predict the positive class. A model that assigns a random score to a triple will result in a horizontal line based on the distribution of classified triples (positive or negative). Therefore a AUC-PR score of 0.5 is good if 99\% of the classified triples are negative, but if the triples are evenly distributed then a AUC-PC score of 0.5 would indicate that the model classified no better than randomly. This metric could have been used in the study for evaluation of the \gls{kge} models, but was omitted as it requires context for interpretation.

We have used the PCA when generating and evaluating rules, which as we know is merely an approximation of the truth. Just because only one child of a person is listed in Wikidata does not mean that this person does not have more children, these missing children were perhaps just not as famous and thus not mentioned in the database. Expanding on this idea of measuring the ``truth" of rules, rule mining approaches have been especially popular due to their explainablity, and the induced knowledge can lead to interesting new knowledge. It is however also prone to errors that occur when creating rules based on bias or errors in the data. For example, in Wikidata5M 99\% of all football players are men, leading to the erroneous rule that if a person is a football player, then they are a man. This is of course not an accurate rule by our understanding, but it captures the bias in the data. Therefore the fact that rules in this study are only evaluated on PCA confidence is a severe limitation when evaluating their quality.

\section{Further work}
There are a great number of ways to expand upon the work in this thesis. As mentioned in section \ref{experiment_limitations} the main limitations of the experiment itself are due to computational restrictions. The quality of the results could most likely be substantially improve if larger and more interesting \glspl{kg} were used, as when \glspl{kg} are restricted to only six relation types there is a relatively small area to create rules within. The \textit{form} of the rules was also limited. Rules contained no entities, only variables, therefore relevant rules such as
\[wonAward(x, Spellemannprisen) \Rightarrow citizenOf(x, Norway)\]
were never considered. Section \ref{the_amies} explained how AMIE3 \textit{is} capable of mining such rules with instantiated variables, but at the expense of increased runtime, which is the reason why it was not done the experiments.

The RandomBaseline KGE assigns a random score to each triple, so if the candidates are randomly generated, then the extension of a \gls{kg} with the RandomBaseline would be equivalent to adding noise to the data. In the experiments of the present study it was shown that not all original rules were mined when the \gls{kg} was extended with the RandomBaseline. This was however only a mere observation, and the extent to which the addition of noise to the data affects the rules being mined from it would be an interesting topic to do further research on.

As mentioned in the introduction, the process of first applying a KGE and \textit{then} a rule mining algorithm could be swapped. In this reverse approach a set of mined rules would be used to extend the \gls{kg} and thereafter a KGE would be trained on the extended \gls{kg}. Instead of evaluating the eventual rules mined from the extended dataset, one would evaluate the resulting KGEs trained on the extended \gls{kg}. In this way one could perhaps gain new insight on the choice that rule mining algorithms have on the KGE that is trained on the extended data.