\chapter{Introduction}

Knowledge bases that are large and interesting are generally not complete. They may for example be extracted from natural language resources and may contain facts that are wrong or exhibit gaps in their knowledge. Most of the data that is present, however, is correct and hence implicitly contain meaningful rules. For example the Wikidata dataset that contains information about many individuals will implicitly contain the fact that siblings tend to have the same mother:
\[siblingOf(a, b) \wedge motherOf(c, a) \Rightarrow motherOf(c, b)\]
There will of course be exceptions to this rule, but generally it will hold. The rules describe information about relational data, and hence will only use binary predicates. The rules will also be Horn, meaning that any number of predicates may be used in the body of the rule, but only one predicate is implied in the rule. This form has useful properties in knowledge representation and reasoning. When extracting rules from knowledge graphs they therefore tend to be Horn rules on binary predicates.

While many KG embedding techniques are both accurate and scalable, one of the main issues with this approach in that results usually are not explainable \cite{bonatti2019knowledge}. Rule-based machine learning approaches provide an explanation in the form of the rules used to make a prediction.  Meilicke et al. showed that a rule-bases mined with AMIE+ are good competitors and often outperform embedding models \cite{ensemble}. This was caused by the fact that the standard benchmark datasets, such as WN18 and FB15k,  have a great deal of relational regularities, such as symmetry and equivalence. Meilicke et al. also compared the two approaches for triple prediction and found that they compliment each other. An ensemble of the two families of approaches gave better results than either of the two alone.

This work inspired the idea of using KG embedding models to improve rule-bases, or vice versa. After applying one KG completion technique, the other can be modeled on the improved KG, resulting in a better model. This can be done both ways, as shown in figure \ref{rule_based_and_embedding}. A rule-base as end-result was chosen, ultimately because it results in an explainable model. 

\begin{figure}[htp]
    \centering
    \includesvg[inkscapelatex=false,width=1\textwidth,keepaspectratio]{figures/intro/custergraf-nesten.svg}
    \caption[Figure representing the process.]{The two versions of KG improvement for better model creation. In the top version an embedding of the original KG is used to improve the graph, from which a rule base is mined. Red edges represent new links made by the models, which originally weren't present in the KG. The second part of the figure represents the same process, but with the rule base used to improve the KG, resulting in an embedding of the improved KG. }
    \label{rule_based_and_embedding}
\end{figure}

This thesis assumes that the reader is familiar with basic machine learning concepts, such as training, testing and overfitting. For an introduction or refresher to machine learning basics, chapter 5 in the book \textit{Deep Learning}, by Goodfellow et al. \cite{goodfellow} covers the topic quite well. The entire book is publicly available at \href{https://www.deeplearningbook.org/}{https://www.deeplearningbook.org/}.