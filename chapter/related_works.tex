\chapter{Related Works}

ONTOLOGICAL PATH FINDING
Chen, Y., Goldberg, S., Wang, D.Z., Johri, S.S.: Ontological Pathfinding. In: SIGMOD (2016) 
4. Chen, Y., Wang, D.Z., Goldberg, S.: ScaLeKB: Scalable Learning and Inference over Large Knowledge Bases. VLDB Journal 25(6) (2016)

\section{Section placeholder 1: works on knowledge graph embeddings}
A knowledge graph completion family that was not explored in this thesis is path-based reasoning methods. One of the first of these was the \textit{path ranking algorithm} (PRA) \cite{lao2011random}. This approach trains a binary classifier for each relation $r$ in the KG, that determines if given two entities $h$ and $t$ is they are connected via $r$. Path ranking algorithms struggle with sparseness in KGs \cite{ma2019elpkg}, and so there have been attempts at combining PRAs and embedding methods. An example of this is PTransE, which considers relation paths with multiple steps, and uses embedding methods similar to TransE to represent these paths \cite{lin2015modeling}. 

TODO: write about translation-based distance model vs semantic matching models

\section{Section placeholder 2: works on rule mining approaches}
Traditional rule mining methods find rules of quality by statistically evaluating support and confidence on candidate rules. AMIE and it's successors are part of this group, and have for a while been considered at the forefront of both efficiency and quality when it comes to mining first-order Horn rules from knowledge graphs. Exact statistical evaluation of rules is expensive, and so there have been approaches adopting embedding methods to score rules \cite{yang2014embedding. omran2018scalable, omran2019embedding}. The rule miner AnyBURL uses rule generation methods different from AMIE, where rules are generated by sampling paths in the KG \cite{meilicke2020reinforced}. These rules are evaluated with the same metrics as AMIE, but only approximated based on sampling in contrast to the exact evaluations in AMIE3. AnyBURL introduced the use of reinforcement learning to improve their sampling, leading to higher quality rules being found earlier in the search. A more recent approach to rule mining with reinforcement learning takes advantage of embedding information to achieve better performance and allows scalable mining of long rules \cite{chen2022rule}. This method also outperforms AMIE+, and the authors point out that the many optimizations made to AMIE+ (resulting in AMIE3) could be applied to any top-down rule mining methods, including theirs.  TODO: explain top down vs bottom up

https://arxiv.org/pdf/2004.04412.pdf

\section{Section placeholder 3: works that combine rule mining and KG embeddings}
Meilicke fuck yeah
https://openreview.net/forum?id=JQHqeGx6qFw