\chapter{Related Work}
\section{Section placeholder 1: works on knowledge graph embeddings}

A wide range of statistical based relational learning techniques have been proposed for KG completion \cite{nickel2015review}. Of these methods vector space embedding approaches one of the most successful due to their 
their performance and scalability, however one of the main problems, as with many other statistical machine learning methods, is that the results are not explainable \cite{bonatti2019knowledge}. Rule based approaches alleviate this problem to some degree. Early works within vector space embeddings include TransE \cite{TransE} and DistMult \cite{yang2014embedding}, which employ simple vector space operations for link prediction. These simple models are capable of handling large scale knowledge graphs, but this is often at the cost of expressiveness \cite{dettmers2018convolutional}. There have been many attempts at increasing expressiveness while maintaining simplicity, such as SimplE \cite{SimplE}, HolE \cite{holE}, and RotatE \cite{rotatE}. A more direct approach for increasing expressiveness is the employment of deep neural networks. This has been done using multi-layer perceptrons \cite{dong2014knowledge}, semantic matching energy networks \cite{bordes2014semantic} and neural tensor networks \cite{socher2013reasoning}. It has however been shown that these approaches have more parameters and are prone to overfit \cite{nickel2015review}. Improving upon this, there have been many approaches using convolutional neural networks \cite{dettmers2018convolutional, nguyen2017novel, jiang2019adaptive}.

TODO: WHAT IS STATE OF THE ART NOW??


A knowledge graph completion family that was not explored in this thesis is path-based reasoning methods. One of the first of these was the \textit{path ranking algorithm} (PRA) \cite{lao2011random}. This approach trains a binary classifier for each relation $r$ in the KG, that given two entities $h$ and $t$ determines if they are connected via a relation $r$. Path ranking algorithms struggle with sparseness in KGs \cite{ma2019elpkg}, and so there have been attempts at combining PRAs and embedding methods. An example of this is PTransE, which considers relation paths with multiple steps, and uses embedding methods similar to TransE to represent these paths \cite{lin2015modeling}. 


\section{Section placeholder 2: works on rule mining approaches}
Traditional rule mining methods find rules of quality by statistically evaluating support and confidence on candidate rules. AMIE and it's successors are part of this group, and have for a while been considered at the forefront of both efficiency and quality when it comes to mining first-order Horn rules from knowledge graphs. Exact statistical evaluation of rules is expensive, and so there have been approaches adopting embedding methods to score rules \cite{yang2014embedding, omran2018scalable, omran2019embedding}. The rule miner AnyBURL uses rule generation methods different from AMIE, where rules are generated by sampling paths in the KG \cite{meilicke2020reinforced}. These rules are evaluated with the same metrics, but scores are approximated based on sampling in contrast to the exact evaluation done in AMIE3. AnyBURL introduced the use of reinforcement learning to improve their sampling, leading to higher quality rules being found earlier in the search. A more recent approach to rule mining with reinforcement learning takes advantage of embedding information to achieve better performance and allows scalable mining of long rules \cite{chen2022rule}. This method also outperforms AMIE+, and the authors point out that the many optimizations made to AMIE+ (resulting in AMIE3) could be applied to any top-down rule mining methods, including theirs. Top-down in this context would be beginning with the most general rules and then expanding on them, essentially performing a search in the space of possible rules. The bottom-up approach on the other hand works from the data and up, starting with very spesific rules and then generalizing them.

Rule mining approaches have been especially popular due to their explainablity, and the induced knowledge can lead to interesting new knowledge. It is however also prone to errors that occur when creating rules based on bias or errors in the data. For example, in Wikidata5M 99\% of all football players are men, leading to the erroneous rule that if a person is a football player, then they are a man. This is of course not an accurate rule by our understanding, but it captures the bias in the data.


\section{Section placeholder 3: works that combine rule mining and KG embeddings}
When it comes to combining the rule based and embedding based approach there seem to be two main ideas for doing this. One of these is to tightly integrate the two approaches, examples of these are KALE \cite{KALE} and RUGE \cite{RUGE}. It has however been noted that these approaches generally do not allow for rules that contain constants \cite{meilicke2021naive}. The second idea proposes to combine the rule based and embedding models into an ensemble. This has been studied by Wang et al. \cite{wang2018multi} and was the focus of the paper by Meillicke et al.\cite{ensemble} mentioned in the introduction. Meillicke et al. extended their work in 2021 with the paper "\textit{Why a Naive Way to Combine Symbolic and Latent Knowledge Base Completion Works Surprisingly Well}" \cite{meilicke2021naive}. The work in this thesis combines rule based and embedding models in a different way, where the information gained by one model is passed on to the next in the form of an extended dataset. While the first model influences the outcome of the next, it does not play a direct role in the final outcome, in this case the outcome being the rules mined from the KG.

