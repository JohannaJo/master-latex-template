{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DBpedia rule suggestions\n",
    "\n",
    "\n",
    "This notebook presents some possible rules that could be extacted from Wikidata. I have limited the rules to those with predicates:\n",
    "- child\n",
    "- parent\n",
    "- relative\n",
    "- spouse\n",
    "\n",
    "*Clarification*: predicate \"parent\" means \"hasParent\", \"child\" means \"hasChild\", \"spouse\" means \"hasSpouse\" etc.\n",
    "\n",
    "---\n",
    "### **Rule 1**: spouse(A, B) -> spouse(B, A)\n",
    "\"If A has spouse B, then B has spouse A.\"\n",
    "\n",
    "Example of supporting datapoints: https://rudik.eurecom.fr/rules/5c6ebe8798254a6e169088cb/sample-instances\n",
    "\n",
    "---\n",
    "### **Rule 2**: child(A, B) & parent(C, A) & child(D, C) -> child(D, B)\n",
    "\"If A has child B, C has parent A and D has child C, then D also has child B.\"\n",
    "\n",
    "Example of supporting datapoints: https://rudik.eurecom.fr/rules/5c852cea7c97766c4b7f4861/sample-instances\n",
    "\n",
    "---\n",
    "### **Rule 3**: parent(A, B) -> child(B, A)\n",
    "\"If A has parent B, then B has child A\"\n",
    "\n",
    "Example of supporting datapoints: https://rudik.eurecom.fr/rules/5d91f3884f01a85b4e9a0feb/sample-instances\n",
    "\n",
    "---\n",
    "### **Rule 4**: relative(A, B) & relative(B, C) & relative(C, A) -> relative(A, C)\n",
    "If A has relative B, B has relative C and C has relative A, then A has relative C.\n",
    "\n",
    "Example of supporting datapoints: https://rudik.eurecom.fr/rules/5c8c14267c97768129b0700b/sample-instances\n",
    "\n",
    "---\n",
    "\n",
    "### **Rule 5**: relative(A, B) & child(C, A) & relative(B, C) -> relative(B, A)\n",
    "If A has relative B, C has child A and B has relative C, then B has relative A.\n",
    "\n",
    "Example of supporting datapoints: https://rudik.eurecom.fr/rules/5c8cb6bd7c97762b388e2bb2/sample-instances\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Rule 6**: child(A, B) & child(A, C) & child(D, C) -> child(D, B)\n",
    "if A has child B, A has child C and D has child C, then D also has child B.\n",
    "\n",
    "Example of supporting datapoints: https://rudik.eurecom.fr/rules/5c863ae27c97762bd32f5d6c/sample-instances\n",
    "\n",
    "---\n",
    "\n",
    "### **Rule 7**: parent(A, B) & parent(B, C) & child(D, C) -> child(D, A)\n",
    "If A has parent B, C has parent B and D has child C, then D also has child A.\n",
    "\n",
    "Example of supporting datapoints: https://rudik.eurecom.fr/rules/5c851b4f7c97766c4b7f484a/sample-instances\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Rule 8**: parent(A, B) & child(B, C) & child(D, C) -> child(C, B)\n",
    "If A has parent B, C has child B and D has child C, then D also has child A.\n",
    "\n",
    "This rule implies incest, but seems to be supported by many datapoints: https://rudik.eurecom.fr/rules/5c8a89ef7c97761b81c6f44a/sample-instances\n",
    "\n",
    "---\n",
    "### **Rule 9**: parent(A, B) & child(C, A) -> spouse(A, C)\n",
    "If A has parent B, and C has child A, then A has spouse C.\n",
    "Example of supporting datapoints: https://rudik.eurecom.fr/rules/5c6ec05398254a6e169088e2/sample-instances\n",
    "\n",
    "\n",
    "---\n",
    "### **Rule 10**: spouse(A, B) & spouse(C, B) & spouse(C, D) -> (A, D)\n",
    "If A has spouse B, C has spouse B and C has spouse D, then A has spouse D.\n",
    "\n",
    "(Seems like A = C and B = D)\n",
    "\n",
    "Example of supporting datapoints: https://rudik.eurecom.fr/rules/5d8887714f01a811d0065f5d/sample-instances\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### **Rule 11**: relagive(A, B) -> relative(B, A)\n",
    "If A has relative B, then B has relative A.\n",
    "\n",
    "This was not listed as an extracted rule on RuleHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.datasets import DBpedia50, DB100K\n",
    "import pykeen\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from signature_tools import subset_by_signature, subset_by_strict_signature, subset_by_frequency, most_frequent_objects, most_frequent_predicates, most_frequent_targets\n",
    "from ampligraph.evaluation import train_test_split_no_unseen \n",
    "from ampligraph.latent_features import ComplEx\n",
    "from ampligraph.latent_features import save_model\n",
    "from ampligraph.evaluation import evaluate_performance\n",
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "from ampligraph.latent_features import save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBpedia50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 11247 entities and 22 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 8871 from 10969 triples were filtered out\n",
      "You're trying to map triples with 332 entities and 1 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 276 from 399 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBpedia50 (create_inverse_triples=False)\n",
      "Name        Entities    Relations      Triples\n",
      "----------  ----------  -----------  ---------\n",
      "Training    24624       351              32203\n",
      "Testing     24624       351               2095\n",
      "Validation  24624       351                123\n",
      "Total       -           -                34421\n",
      "Head                                Relation     tail\n",
      "----------------------------------  -----------  ----------------\n",
      "$_(film)                            starring     Goldie_Hawn\n",
      "&ME                                 language     English_language\n",
      "'Cause_I'm_a_Man                    recordedIn   Fremantle\n",
      "(Ain't_Nobody_Loves_You)_Like_I_Do  genre        Dance_music\n",
      "(Ain't_Nobody_Loves_You)_Like_I_Do  recordLabel  RCA_Records\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = DBpedia50()\n",
    "data.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n"
     ]
    }
   ],
   "source": [
    "# extract all the data into a numpy array of triples\n",
    "train_data = data.training.triples\n",
    "test_data = data.testing.triples\n",
    "validation_data = data.validation.triples\n",
    "data_DBpedia50 = np.concatenate((train_data, test_data, validation_data))\n",
    "data_DBpedia50 = data_DBpedia50.astype('object') # used to have datatype '<U95' which was problematic for signature_tools functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 'List_of_Cypriot_football_transfers_summer_2012'],\n",
       "       [24, 'List_of_Iranian_football_transfers_summer_2012'],\n",
       "       [23, 'List_of_Russian_football_transfers_summer_2013'],\n",
       "       [19, 'List_of_Serbian_football_transfers_winter_2012–13'],\n",
       "       [16, 'List_of_Russian_football_transfers_summer_2009'],\n",
       "       [15, 'List_of_Cypriot_football_transfers_summer_2008'],\n",
       "       [12, 'List_of_Serbian_football_transfers_winter_2009–10'],\n",
       "       [12, 'List_of_Iranian_football_transfers_winter_2014–15'],\n",
       "       [11, 'Nat_Powers'],\n",
       "       [11, 'Ennio_Morricone']], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_objects(data_DBpedia50, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3185, 'team'],\n",
       "       [3033, 'genre'],\n",
       "       [2536, 'birthPlace'],\n",
       "       [1145, 'recordLabel'],\n",
       "       [1080, 'starring'],\n",
       "       [986, 'language'],\n",
       "       [932, 'producer'],\n",
       "       [793, 'class'],\n",
       "       [774, 'associatedBand'],\n",
       "       [774, 'associatedMusicalArtist']], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_predicates(data_DBpedia50, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[854, 'Germany'],\n",
       "       [755, 'English_language'],\n",
       "       [711, 'Hip_hop_music'],\n",
       "       [650, 'London'],\n",
       "       [565, 'Insect'],\n",
       "       [499, 'Plant'],\n",
       "       [478, 'Flowering_plant'],\n",
       "       [466, 'Jazz'],\n",
       "       [390, 'Iran'],\n",
       "       [378, 'Soviet_Union']], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_targets(data_DBpedia50, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_subset_DBpedia50 = subset_by_signature(data_DBpedia50, [], ['child', 'parent', 'relative', 'spouse'], [])\n",
    "family_subset_DBpedia50.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB100K (create_inverse_triples=False)\n",
      "Name        Entities    Relations      Triples\n",
      "----------  ----------  -----------  ---------\n",
      "Training    99604       470             597482\n",
      "Testing     99604       470              50000\n",
      "Validation  99604       470              49997\n",
      "Total       -           -               697479\n",
      "Head    Relation        tail\n",
      "------  --------------  --------\n",
      "Q100    governmentType  Q3308596\n",
      "Q100    isPartOf        Q1191350\n",
      "Q100    isPartOf        Q179876\n",
      "Q100    isPartOf        Q2079909\n",
      "Q100    isPartOf        Q54072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unprocessed_DB100K= DB100K()\n",
    "unprocessed_DB100K.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n"
     ]
    }
   ],
   "source": [
    "# extract all the data into a numpy array of triples\n",
    "train_DB100K = unprocessed_DB100K.training.triples\n",
    "test_DB100K = unprocessed_DB100K.testing.triples\n",
    "validation_DB100K = unprocessed_DB100K.validation.triples\n",
    "data_DB100K = np.concatenate((train_DB100K, test_DB100K, validation_DB100K))\n",
    "data_DB100K = data_DB100K.astype('object') # used to have datatype '<U95' which was problematic for signature_tools functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[115, 'Q323544'],\n",
       "       [108, 'Q5281946'],\n",
       "       [87, 'Q1382555'],\n",
       "       [79, 'Q587361'],\n",
       "       [79, 'Q1370642'],\n",
       "       [76, 'Q158641'],\n",
       "       [71, 'Q1849210'],\n",
       "       [70, 'Q17507684'],\n",
       "       [70, 'Q375792'],\n",
       "       [69, 'Q541659']], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_objects(data_DB100K, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63215, 'genre'],\n",
       "       [52175, 'associatedBand'],\n",
       "       [52174, 'associatedMusicalArtist'],\n",
       "       [40512, 'birthPlace'],\n",
       "       [32992, 'recordLabel'],\n",
       "       [26946, 'country'],\n",
       "       [23630, 'isPartOf'],\n",
       "       [18832, 'occupation'],\n",
       "       [17281, 'hometown'],\n",
       "       [16273, 'instrument']], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_predicates(data_DB100K, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14767, 'Q30'],\n",
       "       [3379, 'Q99'],\n",
       "       [3208, 'Q729'],\n",
       "       [3085, 'Q145'],\n",
       "       [2899, 'Q21'],\n",
       "       [2749, 'Q37073'],\n",
       "       [2462, 'Q11366'],\n",
       "       [2379, 'Q16'],\n",
       "       [2315, 'Q36'],\n",
       "       [2313, 'Q6607']], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_targets(data_DB100K, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_subset = subset_by_signature(data_DB100K, [], ['child', 'parent', 'relative', 'spouse'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4151, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Q100440', 'parent', 'Q285483'],\n",
       "       ['Q1009495', 'spouse', 'Q3290693'],\n",
       "       ['Q1016897', 'parent', 'Q2793470'],\n",
       "       ...,\n",
       "       ['Q984634', 'child', 'Q3351697'],\n",
       "       ['Q994657', 'relative', 'Q3042063'],\n",
       "       ['Q9960', 'child', 'Q321846']], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split_no_unseen(family_subset, test_size=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now split into train/test sets. If we need to further divide into a validation dataset we can just repeat using the same procedure on the test set (and adjusting the split percentages). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  (4051, 3)\n",
      "Test set size:  (100, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"Trondheim_Airport\", \"hasGender\", \"Brisbane_Airport\"] in family_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Training a model \n",
    "\n",
    "AmpliGraph has implemented [several Knoweldge Graph Embedding models](https://docs.ampligraph.org/en/latest/ampligraph.latent_features.html#knowledge-graph-embedding-models) (TransE, ComplEx, DistMult, HolE), but to begin with we're just going to use the [ComplEx](https://docs.ampligraph.org/en/latest/generated/ampligraph.latent_features.ComplEx.html#ampligraph.latent_features.ComplEx) model (with  default values), so lets import that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComplEx(batches_count=100, \n",
    "                seed=0, \n",
    "                epochs=200, \n",
    "                k=150, \n",
    "                eta=5,\n",
    "                optimizer='adam', \n",
    "                optimizer_params={'lr':1e-3},\n",
    "                loss='multiclass_nll', \n",
    "                regularizer='LP', \n",
    "                regularizer_params={'p':3, 'lambda':1e-5}, \n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering negatives\n",
    "\n",
    "AmpliGraph aims to follow scikit-learn's ease-of-use design philosophy and simplify everything down to **`fit`**, **`evaluate`**, and **`predict`** functions. \n",
    "\n",
    "However, there are some knowledge graph specific steps we must take to ensure our model can be trained and evaluated correctly. The first of these is defining the filter that will be used to ensure that no *negative* statements generated by the corruption procedure are actually positives. This is simply done by concatenating our train and test sets. Now when negative triples are generated by the corruption strategy, we can check that they aren't actually true statements.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_filter = family_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "Once you run the next cell the model will train. \n",
    "\n",
    "On a modern laptop this should take ~3 minutes (although your mileage may vary, especially if you've changed any of the hyper-parameters above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average ComplEx Loss:   0.003096: 100%|███| 200/200 [08:21<00:00,  2.51s/epoch]\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "model.fit(X_train, early_stopping = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, './DBpedia_family_subset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save the model in the ampligraph_tutorial directory as `best_model.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. we can then delete the model .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and then restore it from disk! Ta-da! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = restore_model('./connectedTo_subset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's just double check that the model we restored has been fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is fit!\n"
     ]
    }
   ],
   "source": [
    "if model.is_fitted:\n",
    "    print('The model is fit!')\n",
    "else:\n",
    "    print('The model is not fit! Did you skip a step?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:04<00:00, 24.54it/s]\n"
     ]
    }
   ],
   "source": [
    "ranks = evaluate_performance(X_test, \n",
    "                             model=model, \n",
    "                             filter_triples=positives_filter,   # Corruption strategy filter defined above \n",
    "                             use_default_protocol=True, # corrupt subj and obj separately while evaluating\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.84\n",
      "Hits@10: 0.91\n",
      "Hits@3: 0.89\n",
      "Hits@1: 0.79\n"
     ]
    }
   ],
   "source": [
    "mrr = mrr_score(ranks)\n",
    "print(\"MRR: %.2f\" % (mrr))\n",
    "\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"Hits@10: %.2f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(ranks, n=3)\n",
    "print(\"Hits@3: %.2f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(ranks, n=1)\n",
    "print(\"Hits@1: %.2f\" % (hits_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
