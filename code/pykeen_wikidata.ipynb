{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Wikidata rule suggestions\n",
    "\n",
    "\n",
    "This notebook presents some possible rules that could be extacted from Wikidata.\n",
    "\n",
    "Note that Wikidata and DBpedia switch the position of the object and subject.\n",
    "So **P40[Child](A, B)** in Wikidata means that A is the child of B, while in DBpedia **child(A, B)** means that A has child B.\n",
    "\n",
    "---\n",
    "### **Rule 1**: P26[Spouse](A, B) -> P26[Spouse](B, A)\n",
    "\"If A is the spouse of B, then B is a spouse of A.\"\n",
    "\n",
    "---\n",
    "### **Rule 2**: P25[Mother](A, B) -> P40[Child](B, A)\n",
    "\"If A is the mother of B, then B is the child of A\"\n",
    "\n",
    "---\n",
    "### **Rule 3**: p22[Father](A, B) -> P40[Child](B, A)\n",
    "\"If A is the father of B, then B is a child of A.\"\n",
    "\n",
    "---\n",
    "### **Rule 4**: P185[doctoralStudent](A, B) -> P184[doctoralAdvisor](B, A)\n",
    "\"If A is the doctoral student of B, then B is the doctoral advisor of A.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Rule 5**: P800[notableWork](A, B) -> P170[creator](B, A)\n",
    "\"If A is the notable work of B, then B is the creator of A.\"\n",
    "\n",
    "---\n",
    "### **Rule 6**: P1196[mannerOfDeath](A, Q171558[accident]) -> P184[causeOfDeath](A, Q171558[accident])\n",
    "\"If the manner of A's death was an accident, then the cause of A's death was a accident.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.datasets import Wikidata5M\n",
    "import pykeen\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from signature_tools import subset_by_signature, subset_by_strict_signature, subset_by_frequency, most_frequent_objects, most_frequent_predicates, most_frequent_targets\n",
    "from ampligraph.evaluation import train_test_split_no_unseen \n",
    "from ampligraph.latent_features import ComplEx\n",
    "from ampligraph.latent_features import save_model\n",
    "from ampligraph.evaluation import evaluate_performance\n",
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "from ampligraph.latent_features import save_model, restore_model\n",
    "from ampligraph.discovery import discover_facts, find_clusters, find_duplicates, find_nearest_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - You're trying to map triples with 156 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "WARNING - In total 156 from 5133 triples were filtered out\n",
      "WARNING - You're trying to map triples with 180 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "WARNING - In total 180 from 5163 triples were filtered out\n",
      "Wikidata5M (create_inverse_triples=False)\n",
      "Name        Entities    Relations      Triples\n",
      "----------  ----------  -----------  ---------\n",
      "Training    4594149     822           20614279\n",
      "Testing     4594149     822               4977\n",
      "Validation  4594149     822               4983\n",
      "Total       -           -             20624239\n",
      "Head    Relation    tail\n",
      "------  ----------  --------\n",
      "Q1      P1343       Q602358\n",
      "Q1      P1419       Q1647152\n",
      "Q1      P1552       Q11412\n",
      "Q1      P2184       Q136407\n",
      "Q1      P2670       Q18343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_unprocessed = Wikidata5M()\n",
    "data_unprocessed.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "WARNING - Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "WARNING - Reconstructing all label-based triples. This is expensive and rarely needed.\n"
     ]
    }
   ],
   "source": [
    "# extract all the data into a numpy array of triples\n",
    "train_Wikidata = data_unprocessed.training.triples\n",
    "test_Wikidata = data_unprocessed.testing.triples\n",
    "validation_Wikidata = data_unprocessed.validation.triples\n",
    "data_Wikidata = np.concatenate((train_Wikidata, test_Wikidata, validation_Wikidata))\n",
    "data_Wikidata = data_Wikidata.astype('object') # used to have datatype '<U95' which was problematic for signature_tools functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-afa5e10cf62d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmost_frequent_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_Wikidata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Master Machine Learning\\Git clones\\master-thesis-JohannaJo\\code\\signature_tools.py\u001b[0m in \u001b[0;36mmost_frequent_objects\u001b[1;34m(dataset, n)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmost\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0mto\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \"\"\"\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_object_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     \u001b[0msorted_frequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0msorted_frequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_frequencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Master Machine Learning\\Git clones\\master-thesis-JohannaJo\\code\\signature_tools.py\u001b[0m in \u001b[0;36mget_object_frequencies\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_object_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mobjects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0munique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "most_frequent_objects(data_Wikidata, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3841627, 'P31'],\n",
       "       [1375274, 'P17'],\n",
       "       [1146848, 'P27'],\n",
       "       [1101408, 'P106'],\n",
       "       [919905, 'P54'],\n",
       "       [919489, 'P131'],\n",
       "       [856424, 'P19'],\n",
       "       [838752, 'P735'],\n",
       "       [514458, 'P161'],\n",
       "       [485845, 'P641'],\n",
       "       [439909, 'P69'],\n",
       "       [429980, 'P47'],\n",
       "       [404947, 'P421'],\n",
       "       [374369, 'P105'],\n",
       "       [370254, 'P136'],\n",
       "       [348802, 'P171'],\n",
       "       [279197, 'P20'],\n",
       "       [261001, 'P495'],\n",
       "       [238803, 'P1412'],\n",
       "       [222716, 'P1344'],\n",
       "       [183473, 'P175'],\n",
       "       [182448, 'P166'],\n",
       "       [181086, 'P413'],\n",
       "       [154179, 'P264'],\n",
       "       [149046, 'P155'],\n",
       "       [141736, 'P156'],\n",
       "       [136593, 'P364'],\n",
       "       [136331, 'P361'],\n",
       "       [126546, 'P102'],\n",
       "       [108630, 'P150'],\n",
       "       [108322, 'P734'],\n",
       "       [99586, 'P57'],\n",
       "       [96454, 'P463'],\n",
       "       [95780, 'P279'],\n",
       "       [94036, 'P407'],\n",
       "       [82598, 'P159'],\n",
       "       [80172, 'P108'],\n",
       "       [76208, 'P39'],\n",
       "       [75654, 'P3373'],\n",
       "       [71660, 'P937'],\n",
       "       [70475, 'P607'],\n",
       "       [65945, 'P360'],\n",
       "       [65857, 'P527'],\n",
       "       [64983, 'P40'],\n",
       "       [62938, 'P86'],\n",
       "       [59134, 'P162'],\n",
       "       [57908, 'P50'],\n",
       "       [56205, 'P137'],\n",
       "       [52021, 'P141'],\n",
       "       [49973, 'P22'],\n",
       "       [49009, 'P1343'],\n",
       "       [48949, 'P58'],\n",
       "       [46561, 'P26'],\n",
       "       [45602, 'P123'],\n",
       "       [42558, 'P1303'],\n",
       "       [42451, 'P1196'],\n",
       "       [40723, 'P400'],\n",
       "       [39544, 'P190'],\n",
       "       [38340, 'P241'],\n",
       "       [37823, 'P138'],\n",
       "       [35724, 'P344'],\n",
       "       [34314, 'P119'],\n",
       "       [33952, 'P276'],\n",
       "       [33398, 'P127'],\n",
       "       [32983, 'P750'],\n",
       "       [31586, 'P140'],\n",
       "       [31025, 'P1346'],\n",
       "       [30738, 'P462'],\n",
       "       [30229, 'P3450'],\n",
       "       [30121, 'P449'],\n",
       "       [29651, 'P509'],\n",
       "       [29591, 'P176'],\n",
       "       [27917, 'P172'],\n",
       "       [26733, 'P197'],\n",
       "       [25891, 'P840'],\n",
       "       [25185, 'P921'],\n",
       "       [25073, 'P404'],\n",
       "       [23173, 'P179'],\n",
       "       [23061, 'P1411'],\n",
       "       [22537, 'P272'],\n",
       "       [21538, 'P118'],\n",
       "       [21449, 'P180'],\n",
       "       [20785, 'P36'],\n",
       "       [20541, 'P30'],\n",
       "       [20419, 'P101'],\n",
       "       [20212, 'P81'],\n",
       "       [20155, 'P103'],\n",
       "       [18618, 'P706'],\n",
       "       [18559, 'P178'],\n",
       "       [18513, 'P710'],\n",
       "       [17445, 'P170'],\n",
       "       [16711, 'P915'],\n",
       "       [16689, 'P282'],\n",
       "       [16603, 'P149'],\n",
       "       [16421, 'P410'],\n",
       "       [16408, 'P25'],\n",
       "       [15690, 'P84'],\n",
       "       [15643, 'P1435'],\n",
       "       [15518, 'P1889'],\n",
       "       [15179, 'P740']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_predicates(data_Wikidata, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_targets(data_Wikidata, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P8810 = parent of the subject. Only use if neither father (P22) nor mother (P25) is applicable\n",
    "#P24 = child\n",
    "#P26 = spouse\n",
    "# P25 mother of the subject. For stepmother, use \"stepparent\" (P3448)\n",
    "#P1038 = relative\n",
    "#P3373 = sibling\n",
    "#P3448 = stepparent\n",
    "family_predicates_dict = {\n",
    "    'P24': 'child', \n",
    "    'P22' : 'father', \n",
    "    'P25' : 'mother',\n",
    "    'P26' : 'spouse', \n",
    "    'P1038' : 'relative', \n",
    "    'P3373' : 'sibling', \n",
    "    'P3448' : 'stepparent'\n",
    "}\n",
    "family_subset = subset_by_signature(data_Wikidata, [], ['P24', 'P22', 'P25', 'P26', 'P1038', 'P3373', 'P3448'], [])\n",
    "family_subset[:,1] = np.array([family_predicates_dict[p] for p in family_subset[:,1]]) # convert predicates to understandable strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule minig\n",
    "Use AMIE+ to mine rules. Call jar file from https://github.com/lajus/amie using this command:\n",
    "\n",
    "*java -jar amie-dev.jar -bias lazy -full -noHeuristics -ostd [TSV file] > [txt file to save rules in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''UNCOMMENT TO SAVE NEW SUBSET'''\n",
    "pd.DataFrame(family_subset).to_csv(\"family_KB.txt\", sep = \"\\t\", header=None, index=None) # remember to later rename to tsv filetype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual work:\n",
    "\n",
    "1. rename \"family_KB.txt\" to \"family_KB.csv\"\n",
    "2. Mine rules with command:\n",
    "```java -jar amie-milestone-intKB.jar -bias lazy -full -noHeuristics -ostd family_KB.tsv > family_mined_rules.txt```\n",
    "3. Remove useless text so that family_mined_rules.txt only contains the mined rules. Save as family_mined_rules_cleaned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_mining_results = pd.read_csv(\"family_mined_rules_cleaned.txt\", sep=\"\\t\", names=[\"Rule\", \"Head Coverage\", \"PCA Confidence\", \"Positive Examples\", \"PCA Body size\", \"Functional variable\"], skiprows = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rule</th>\n",
       "      <th>Head Coverage</th>\n",
       "      <th>PCA Confidence</th>\n",
       "      <th>Positive Examples</th>\n",
       "      <th>PCA Body size</th>\n",
       "      <th>Functional variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?b  relative  ?a   =&gt; ?a  stepparent  ?b</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?b  relative  ?a   =&gt; ?a  relative  ?b</td>\n",
       "      <td>0.551117</td>\n",
       "      <td>0.846306</td>\n",
       "      <td>2566</td>\n",
       "      <td>3032</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?b  spouse  ?a   =&gt; ?a  spouse  ?b</td>\n",
       "      <td>0.947832</td>\n",
       "      <td>0.992355</td>\n",
       "      <td>44132</td>\n",
       "      <td>44472</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>?b  sibling  ?a   =&gt; ?a  sibling  ?b</td>\n",
       "      <td>0.961588</td>\n",
       "      <td>0.989634</td>\n",
       "      <td>72748</td>\n",
       "      <td>73510</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>?b  relative  ?h  ?a  sibling  ?h   =&gt; ?a  ste...</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>?a  father  ?h  ?b  father  ?h   =&gt; ?a  siblin...</td>\n",
       "      <td>0.773231</td>\n",
       "      <td>0.658865</td>\n",
       "      <td>58498</td>\n",
       "      <td>88786</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>?h  sibling  ?a  ?b  sibling  ?h   =&gt; ?a  sibl...</td>\n",
       "      <td>0.716023</td>\n",
       "      <td>0.584857</td>\n",
       "      <td>54170</td>\n",
       "      <td>92621</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>?h  sibling  ?b  ?a  sibling  ?h   =&gt; ?a  sibl...</td>\n",
       "      <td>0.721059</td>\n",
       "      <td>0.588151</td>\n",
       "      <td>54551</td>\n",
       "      <td>92750</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>?a  sibling  ?h  ?b  sibling  ?h   =&gt; ?a  sibl...</td>\n",
       "      <td>0.718680</td>\n",
       "      <td>0.590059</td>\n",
       "      <td>54371</td>\n",
       "      <td>92145</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>?g  sibling  ?a  ?g  sibling  ?b   =&gt; ?a  sibl...</td>\n",
       "      <td>0.721125</td>\n",
       "      <td>0.564657</td>\n",
       "      <td>54556</td>\n",
       "      <td>96618</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Rule  Head Coverage  \\\n",
       "0            ?b  relative  ?a   => ?a  stepparent  ?b       0.056604   \n",
       "1              ?b  relative  ?a   => ?a  relative  ?b       0.551117   \n",
       "2                  ?b  spouse  ?a   => ?a  spouse  ?b       0.947832   \n",
       "3                ?b  sibling  ?a   => ?a  sibling  ?b       0.961588   \n",
       "4   ?b  relative  ?h  ?a  sibling  ?h   => ?a  ste...       0.018868   \n",
       "..                                                ...            ...   \n",
       "59  ?a  father  ?h  ?b  father  ?h   => ?a  siblin...       0.773231   \n",
       "60  ?h  sibling  ?a  ?b  sibling  ?h   => ?a  sibl...       0.716023   \n",
       "61  ?h  sibling  ?b  ?a  sibling  ?h   => ?a  sibl...       0.721059   \n",
       "62  ?a  sibling  ?h  ?b  sibling  ?h   => ?a  sibl...       0.718680   \n",
       "63  ?g  sibling  ?a  ?g  sibling  ?b   => ?a  sibl...       0.721125   \n",
       "\n",
       "    PCA Confidence  Positive Examples  PCA Body size  Functional variable  \n",
       "0         0.750000                  6              8                   -1  \n",
       "1         0.846306               2566           3032                   -2  \n",
       "2         0.992355              44132          44472                   -1  \n",
       "3         0.989634              72748          73510                   -2  \n",
       "4         0.500000                  2              4                   -1  \n",
       "..             ...                ...            ...                  ...  \n",
       "59        0.658865              58498          88786                   -2  \n",
       "60        0.584857              54170          92621                   -2  \n",
       "61        0.588151              54551          92750                   -2  \n",
       "62        0.590059              54371          92145                   -2  \n",
       "63        0.564657              54556          96618                   -2  \n",
       "\n",
       "[64 rows x 6 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_mining_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract subset contaning 90% of the data\n",
    "random_subset_mask = np.random.choice([False, True], len(family_subset), p=[0.1, 0.9])\n",
    "family_subset_subset = family_subset[random_subset_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''UNCOMMENT TO SAVE NEW SUBSET'''\n",
    "#pd.DataFrame(family_subset_subset).to_csv(\"family_random_subset_KB.txt\", sep = \"\\t\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual work:\n",
    "\n",
    "1. rename \"family_random_subset_KB.txt\" to \"family_random_subset_KB.csv\"\n",
    "2. Mine rules with command: java -jar amie-milestone-intKB.jar -bias lazy -full -noHeuristics -ostd family_random_subset_KB.tsv > family_random_subset_mined_rules.txt\n",
    "3. Remove useless text so that family_random_subset_mined_rules.txt only contains the mined rules. Save as family_random_subset_mined_rules_cleaned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_random_subset_mining_results = pd.read_csv(\"family_random_subset_mined_rules_cleaned.txt\", sep=\"\\t\", names=[\"Rule\", \"Head Coverage\", \"PCA Confidence\", \"Positive Examples\", \"PCA Body size\", \"Functional variable\"], skiprows = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Training a model \n",
    "\n",
    "AmpliGraph has implemented [several Knoweldge Graph Embedding models](https://docs.ampligraph.org/en/latest/ampligraph.latent_features.html#knowledge-graph-embedding-models) (TransE, ComplEx, DistMult, HolE), but to begin with we're just going to use the [ComplEx](https://docs.ampligraph.org/en/latest/generated/ampligraph.latent_features.ComplEx.html#ampligraph.latent_features.ComplEx) model (with  default values), so lets import that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split_no_unseen(family_subset, test_size=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now split into train/test sets. If we need to further divide into a validation dataset we can just repeat using the same procedure on the test set (and adjusting the split percentages). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  (193258, 3)\n",
      "Test set size:  (100, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComplEx(batches_count=100, \n",
    "                seed=0, \n",
    "                epochs=200, \n",
    "                k=150, \n",
    "                eta=5,\n",
    "                optimizer='adam', \n",
    "                optimizer_params={'lr':1e-3},\n",
    "                loss='multiclass_nll', \n",
    "                regularizer='LP', \n",
    "                regularizer_params={'p':3, 'lambda':1e-5}, \n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering negatives\n",
    "\n",
    "AmpliGraph aims to follow scikit-learn's ease-of-use design philosophy and simplify everything down to **`fit`**, **`evaluate`**, and **`predict`** functions. \n",
    "\n",
    "However, there are some knowledge graph specific steps we must take to ensure our model can be trained and evaluated correctly. The first of these is defining the filter that will be used to ensure that no *negative* statements generated by the corruption procedure are actually positives. This is simply done by concatenating our train and test sets. Now when negative triples are generated by the corruption strategy, we can check that they aren't actually true statements.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_filter = family_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "Once you run the next cell the model will train. \n",
    "\n",
    "On a modern laptop this should take ~3 minutes (although your mileage may vary, especially if you've changed any of the hyper-parameters above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average ComplEx Loss:   0.000595: 100%|█| 200/200 [4:25:02<00:00, 79.51s/epoch]\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "model.fit(X_train, early_stopping = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, './Wikidata_family_subset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save the model in the ampligraph_tutorial directory as `best_model.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. we can then delete the model .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and then restore it from disk! Ta-da! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = restore_model('./Wikidata_family_subset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ampligraph.latent_features.models.ComplEx.ComplEx at 0x1f3caec3788>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ampligraph.latent_features.models.ComplEx.ComplEx at 0x1f3c85f8a08>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's just double check that the model we restored has been fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is fit!\n"
     ]
    }
   ],
   "source": [
    "if model.is_fitted:\n",
    "    print('The model is fit!')\n",
    "else:\n",
    "    print('The model is not fit! Did you skip a step?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n",
      "WARNING - You are attempting to use 114726 distinct entities to generate synthetic negatives in the evaluation\n",
      "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
      "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
      "    of entities. The size of corruption_entities depends on your domain-specific task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\johan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ampligraph\\evaluation\\protocol.py:952: UserWarning: You are attempting to use 114726 distinct entities to generate synthetic negatives in the evaluation\n",
      "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
      "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
      "    of entities. The size of corruption_entities depends on your domain-specific task.\n",
      "  warnings.warn(warn_msg % ent_for_corruption_size)\n",
      "100%|████████████████████████████████████████| 100/100 [01:40<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "ranks = evaluate_performance(X_test, \n",
    "                             model=model, \n",
    "                             filter_triples=positives_filter,   # Corruption strategy filter defined above \n",
    "                             use_default_protocol=True, # corrupt subj and obj separately while evaluating\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.85\n",
      "Hits@10: 0.93\n",
      "Hits@3: 0.91\n",
      "Hits@1: 0.78\n"
     ]
    }
   ],
   "source": [
    "mrr = mrr_score(ranks)\n",
    "print(\"MRR: %.2f\" % (mrr))\n",
    "\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"Hits@10: %.2f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(ranks, n=3)\n",
    "print(\"Hits@3: %.2f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(ranks, n=1)\n",
    "print(\"Hits@1: %.2f\" % (hits_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsubset = family_subset[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n",
      "WARNING - You are attempting to use 114726 distinct entities to generate synthetic negatives in the evaluation\n",
      "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
      "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
      "    of entities. The size of corruption_entities depends on your domain-specific task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [01:40<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n",
      "WARNING - You are attempting to use 114726 distinct entities to generate synthetic negatives in the evaluation\n",
      "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
      "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
      "    of entities. The size of corruption_entities depends on your domain-specific task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [01:52<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n",
      "WARNING - You are attempting to use 114726 distinct entities to generate synthetic negatives in the evaluation\n",
      "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
      "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
      "    of entities. The size of corruption_entities depends on your domain-specific task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [01:51<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n",
      "WARNING - You are attempting to use 114726 distinct entities to generate synthetic negatives in the evaluation\n",
      "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
      "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
      "    of entities. The size of corruption_entities depends on your domain-specific task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [01:53<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n",
      "WARNING - You are attempting to use 114726 distinct entities to generate synthetic negatives in the evaluation\n",
      "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
      "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
      "    of entities. The size of corruption_entities depends on your domain-specific task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [01:44<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n",
      "WARNING - You are attempting to use 114726 distinct entities to generate synthetic negatives in the evaluation\n",
      "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
      "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
      "    of entities. The size of corruption_entities depends on your domain-specific task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [01:44<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = discover_facts(family_subset, model, top_n = 3, max_candidates = 100, seed = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], shape=(0, 18), dtype=object), array([], dtype=float64))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 18), dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
