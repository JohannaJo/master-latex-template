{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Wikidata rule suggestions\n",
    "\n",
    "\n",
    "This notebook presents some possible rules that could be extacted from Wikidata.\n",
    "\n",
    "Note that Wikidata and DBpedia switch the position of the object and subject.\n",
    "So **P40[Child](A, B)** in Wikidata means that A is the child of B, while in DBpedia **child(A, B)** means that A has child B.\n",
    "\n",
    "---\n",
    "### **Rule 1**: P26[Spouse](A, B) -> P26[Spouse](B, A)\n",
    "\"If A is the spouse of B, then B is a spouse of A.\"\n",
    "\n",
    "---\n",
    "### **Rule 2**: P25[Mother](A, B) -> P40[Child](B, A)\n",
    "\"If A is the mother of B, then B is the child of A\"\n",
    "\n",
    "---\n",
    "### **Rule 3**: p22[Father](A, B) -> P40[Child](B, A)\n",
    "\"If A is the father of B, then B is a child of A.\"\n",
    "\n",
    "---\n",
    "### **Rule 4**: P185[doctoralStudent](A, B) -> P184[doctoralAdvisor](B, A)\n",
    "\"If A is the doctoral student of B, then B is the doctoral advisor of A.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Rule 5**: P800[notableWork](A, B) -> P170[creator](B, A)\n",
    "\"If A is the notable work of B, then B is the creator of A.\"\n",
    "\n",
    "---\n",
    "### **Rule 6**: P1196[mannerOfDeath](A, Q171558[accident]) -> P184[causeOfDeath](A, Q171558[accident])\n",
    "\"If the manner of A's death was an accident, then the cause of A's death was a accident.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.datasets import Wikidata5M\n",
    "import pykeen\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from signature_tools import subset_by_signature, subset_by_strict_signature, subset_by_frequency, most_frequent_objects, most_frequent_predicates, most_frequent_targets\n",
    "from ampligraph.evaluation import train_test_split_no_unseen \n",
    "from ampligraph.latent_features import ComplEx\n",
    "from ampligraph.latent_features import save_model\n",
    "from ampligraph.evaluation import evaluate_performance\n",
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "from ampligraph.latent_features import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 156 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 156 from 5133 triples were filtered out\n",
      "You're trying to map triples with 180 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 180 from 5163 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikidata5M (create_inverse_triples=False)\n",
      "Name        Entities    Relations      Triples\n",
      "----------  ----------  -----------  ---------\n",
      "Training    4594149     822           20614279\n",
      "Testing     4594149     822               4977\n",
      "Validation  4594149     822               4983\n",
      "Total       -           -             20624239\n",
      "Head    Relation    tail\n",
      "------  ----------  --------\n",
      "Q1      P1343       Q602358\n",
      "Q1      P1419       Q1647152\n",
      "Q1      P1552       Q11412\n",
      "Q1      P2184       Q136407\n",
      "Q1      P2670       Q18343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_unprocessed = Wikidata5M()\n",
    "data_unprocessed.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n"
     ]
    }
   ],
   "source": [
    "# extract all the data into a numpy array of triples\n",
    "train_Wikidata = data_unprocessed.training.triples\n",
    "test_Wikidata = data_unprocessed.testing.triples\n",
    "validation_Wikidata = data_unprocessed.validation.triples\n",
    "data_Wikidata = np.concatenate((train_Wikidata, test_Wikidata, validation_Wikidata))\n",
    "data_Wikidata = data_Wikidata.astype('object') # used to have datatype '<U95' which was problematic for signature_tools functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-afa5e10cf62d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmost_frequent_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_Wikidata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Master Machine Learning\\Git clones\\master-thesis-JohannaJo\\code\\signature_tools.py\u001b[0m in \u001b[0;36mmost_frequent_objects\u001b[1;34m(dataset, n)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmost\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0mto\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \"\"\"\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_object_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     \u001b[0msorted_frequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0msorted_frequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_frequencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Master Machine Learning\\Git clones\\master-thesis-JohannaJo\\code\\signature_tools.py\u001b[0m in \u001b[0;36mget_object_frequencies\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_object_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mobjects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0munique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "most_frequent_objects(data_Wikidata, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3841627, 'P31'],\n",
       "       [1375274, 'P17'],\n",
       "       [1146848, 'P27'],\n",
       "       [1101408, 'P106'],\n",
       "       [919905, 'P54'],\n",
       "       [919489, 'P131'],\n",
       "       [856424, 'P19'],\n",
       "       [838752, 'P735'],\n",
       "       [514458, 'P161'],\n",
       "       [485845, 'P641'],\n",
       "       [439909, 'P69'],\n",
       "       [429980, 'P47'],\n",
       "       [404947, 'P421'],\n",
       "       [374369, 'P105'],\n",
       "       [370254, 'P136'],\n",
       "       [348802, 'P171'],\n",
       "       [279197, 'P20'],\n",
       "       [261001, 'P495'],\n",
       "       [238803, 'P1412'],\n",
       "       [222716, 'P1344'],\n",
       "       [183473, 'P175'],\n",
       "       [182448, 'P166'],\n",
       "       [181086, 'P413'],\n",
       "       [154179, 'P264'],\n",
       "       [149046, 'P155'],\n",
       "       [141736, 'P156'],\n",
       "       [136593, 'P364'],\n",
       "       [136331, 'P361'],\n",
       "       [126546, 'P102'],\n",
       "       [108630, 'P150'],\n",
       "       [108322, 'P734'],\n",
       "       [99586, 'P57'],\n",
       "       [96454, 'P463'],\n",
       "       [95780, 'P279'],\n",
       "       [94036, 'P407'],\n",
       "       [82598, 'P159'],\n",
       "       [80172, 'P108'],\n",
       "       [76208, 'P39'],\n",
       "       [75654, 'P3373'],\n",
       "       [71660, 'P937'],\n",
       "       [70475, 'P607'],\n",
       "       [65945, 'P360'],\n",
       "       [65857, 'P527'],\n",
       "       [64983, 'P40'],\n",
       "       [62938, 'P86'],\n",
       "       [59134, 'P162'],\n",
       "       [57908, 'P50'],\n",
       "       [56205, 'P137'],\n",
       "       [52021, 'P141'],\n",
       "       [49973, 'P22'],\n",
       "       [49009, 'P1343'],\n",
       "       [48949, 'P58'],\n",
       "       [46561, 'P26'],\n",
       "       [45602, 'P123'],\n",
       "       [42558, 'P1303'],\n",
       "       [42451, 'P1196'],\n",
       "       [40723, 'P400'],\n",
       "       [39544, 'P190'],\n",
       "       [38340, 'P241'],\n",
       "       [37823, 'P138'],\n",
       "       [35724, 'P344'],\n",
       "       [34314, 'P119'],\n",
       "       [33952, 'P276'],\n",
       "       [33398, 'P127'],\n",
       "       [32983, 'P750'],\n",
       "       [31586, 'P140'],\n",
       "       [31025, 'P1346'],\n",
       "       [30738, 'P462'],\n",
       "       [30229, 'P3450'],\n",
       "       [30121, 'P449'],\n",
       "       [29651, 'P509'],\n",
       "       [29591, 'P176'],\n",
       "       [27917, 'P172'],\n",
       "       [26733, 'P197'],\n",
       "       [25891, 'P840'],\n",
       "       [25185, 'P921'],\n",
       "       [25073, 'P404'],\n",
       "       [23173, 'P179'],\n",
       "       [23061, 'P1411'],\n",
       "       [22537, 'P272'],\n",
       "       [21538, 'P118'],\n",
       "       [21449, 'P180'],\n",
       "       [20785, 'P36'],\n",
       "       [20541, 'P30'],\n",
       "       [20419, 'P101'],\n",
       "       [20212, 'P81'],\n",
       "       [20155, 'P103'],\n",
       "       [18618, 'P706'],\n",
       "       [18559, 'P178'],\n",
       "       [18513, 'P710'],\n",
       "       [17445, 'P170'],\n",
       "       [16711, 'P915'],\n",
       "       [16689, 'P282'],\n",
       "       [16603, 'P149'],\n",
       "       [16421, 'P410'],\n",
       "       [16408, 'P25'],\n",
       "       [15690, 'P84'],\n",
       "       [15643, 'P1435'],\n",
       "       [15518, 'P1889'],\n",
       "       [15179, 'P740']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_predicates(data_Wikidata, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_targets(data_Wikidata, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P8810 = parent of the subject. Only use if neither father (P22) nor mother (P25) is applicable\n",
    "#P24 = child\n",
    "#P26 = spouse\n",
    "# P25 mother of the subject. For stepmother, use \"stepparent\" (P3448)\n",
    "#P1038 = relative\n",
    "#P3373 = sibling\n",
    "#P3448 = steparent\n",
    "family_subset = subset_by_signature(data_Wikidata, [], ['P24', 'P22', 'P25', 'P26', 'P1038', 'P3373', 'P3448'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193358, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split_no_unseen(family_subset, test_size=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now split into train/test sets. If we need to further divide into a validation dataset we can just repeat using the same procedure on the test set (and adjusting the split percentages). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  (193258, 3)\n",
      "Test set size:  (100, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"Trondheim_Airport\", \"hasGender\", \"Brisbane_Airport\"] in family_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Training a model \n",
    "\n",
    "AmpliGraph has implemented [several Knoweldge Graph Embedding models](https://docs.ampligraph.org/en/latest/ampligraph.latent_features.html#knowledge-graph-embedding-models) (TransE, ComplEx, DistMult, HolE), but to begin with we're just going to use the [ComplEx](https://docs.ampligraph.org/en/latest/generated/ampligraph.latent_features.ComplEx.html#ampligraph.latent_features.ComplEx) model (with  default values), so lets import that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComplEx(batches_count=100, \n",
    "                seed=0, \n",
    "                epochs=200, \n",
    "                k=150, \n",
    "                eta=5,\n",
    "                optimizer='adam', \n",
    "                optimizer_params={'lr':1e-3},\n",
    "                loss='multiclass_nll', \n",
    "                regularizer='LP', \n",
    "                regularizer_params={'p':3, 'lambda':1e-5}, \n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering negatives\n",
    "\n",
    "AmpliGraph aims to follow scikit-learn's ease-of-use design philosophy and simplify everything down to **`fit`**, **`evaluate`**, and **`predict`** functions. \n",
    "\n",
    "However, there are some knowledge graph specific steps we must take to ensure our model can be trained and evaluated correctly. The first of these is defining the filter that will be used to ensure that no *negative* statements generated by the corruption procedure are actually positives. This is simply done by concatenating our train and test sets. Now when negative triples are generated by the corruption strategy, we can check that they aren't actually true statements.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_filter = family_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "Once you run the next cell the model will train. \n",
    "\n",
    "On a modern laptop this should take ~3 minutes (although your mileage may vary, especially if you've changed any of the hyper-parameters above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average ComplEx Loss:   0.000595: 100%|█| 200/200 [4:25:02<00:00, 79.51s/epoch]\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "model.fit(X_train, early_stopping = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, './Wikidata_family_subset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save the model in the ampligraph_tutorial directory as `best_model.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. we can then delete the model .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and then restore it from disk! Ta-da! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = restore_model('./connectedTo_subset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's just double check that the model we restored has been fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is fit!\n"
     ]
    }
   ],
   "source": [
    "if model.is_fitted:\n",
    "    print('The model is fit!')\n",
    "else:\n",
    "    print('The model is not fit! Did you skip a step?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n",
      "WARNING - You are attempting to use 114726 distinct entities to generate synthetic negatives in the evaluation\n",
      "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
      "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
      "    of entities. The size of corruption_entities depends on your domain-specific task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\johan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ampligraph\\evaluation\\protocol.py:952: UserWarning: You are attempting to use 114726 distinct entities to generate synthetic negatives in the evaluation\n",
      "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
      "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
      "    of entities. The size of corruption_entities depends on your domain-specific task.\n",
      "  warnings.warn(warn_msg % ent_for_corruption_size)\n",
      "100%|████████████████████████████████████████| 100/100 [01:40<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "ranks = evaluate_performance(X_test, \n",
    "                             model=model, \n",
    "                             filter_triples=positives_filter,   # Corruption strategy filter defined above \n",
    "                             use_default_protocol=True, # corrupt subj and obj separately while evaluating\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.85\n",
      "Hits@10: 0.93\n",
      "Hits@3: 0.91\n",
      "Hits@1: 0.78\n"
     ]
    }
   ],
   "source": [
    "mrr = mrr_score(ranks)\n",
    "print(\"MRR: %.2f\" % (mrr))\n",
    "\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"Hits@10: %.2f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(ranks, n=3)\n",
    "print(\"Hits@3: %.2f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(ranks, n=1)\n",
    "print(\"Hits@1: %.2f\" % (hits_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
