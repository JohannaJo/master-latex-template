{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import config\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from rule_mining import rule_mining\n",
    "from kb_extension import extend_kb\n",
    "from ampligraph.latent_features import restore_model\n",
    "from rule_comparison import plot_pie_chart, get_common_rules, display_comparison\n",
    "from operator import itemgetter\n",
    "\n",
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule mining comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original knowlege base\n",
    "original_kb = np.loadtxt(\"family_subset.txt\", dtype = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine rules from original knowledge base\n",
    "original_rules = rule_mining(original_kb)\n",
    "\n",
    "# convert metrics to correct datatype\n",
    "original_rules['PCA Confidence'] = original_rules['PCA Confidence'].apply(lambda x: float(x.replace(',','.')))\n",
    "original_rules['Head Coverage'] = original_rules['Head Coverage'].apply(lambda x: float(x.replace(',','.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rules_median_PCA = original_rules[\"PCA Confidence\"].median()\n",
    "original_rules_median_HC = original_rules[\"Head Coverage\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save rules mined from original kb\n",
    "original_rules.to_pickle(\"./original_rules.pkl\")\n",
    "\n",
    "# load saved rules\n",
    "# original_rules = pd.read_pickle(\"./original_rules.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - All triples will be processed in the same batch (batches_count=1). When processing large graphs it is recommended to batch the input knowledge graph instead.\n"
     ]
    }
   ],
   "source": [
    "# for testing\n",
    "#original_kb = np.loadtxt(\"/testing/family_subset_test.txt\", dtype = 'object')\n",
    "\n",
    "# load pretrained knowledge graph embeddings\n",
    "randomBaseline_model= restore_model('./RandomBaseline.pkl')\n",
    "transE_model= restore_model('./TransE.pkl')\n",
    "distMult_model = restore_model('./DistMult.pkl')\n",
    "complEx_model = restore_model('./ComplEx.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "models = [complEx_model, distMult_model, transE_model, randomBaseline_model]\n",
    "entity_selection_methods = [\"random\", \"probabilistic\", \"most_frequent\", \"least_frequent\"]\n",
    "candidate_admittance_criteria = config.rank_cutoffs + config.percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_combinations= []\n",
    "for model in models:\n",
    "    for method in entity_selection_methods:\n",
    "        for criteria in candidate_admittance_criteria:\n",
    "            model_name = model.name\n",
    "            parameter_combinations.append([model_name, method, criteria])\n",
    "parameter_combinations = pd.DataFrame(parameter_combinations, columns=[\"Model\", \"Entity_selection\", \"Candidate_criteria\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameter combinations to file\n",
    "with open(\"parameter_combinations.pkl\", \"wb\") as file:\n",
    "    pickle.dump(parameter_combinations, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv30/fak006/miniconda/envs/ampligraph/lib/python3.7/site-packages/ampligraph/latent_features/models/EmbeddingModel.py:1329: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if corruption_entities == 'all':\n",
      "100%|██████████| 600/600 [00:00<00:00, 1589.12it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1621.91it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1624.08it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1589.63it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1617.71it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1659.07it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1612.64it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1622.59it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1580.97it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1576.18it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1592.91it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1585.07it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1563.14it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1592.24it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1637.10it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1584.94it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1626.25it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1516.35it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1616.01it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1610.81it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1576.85it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1575.06it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1571.94it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1586.39it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1577.14it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1573.90it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1578.00it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1612.28it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1781.74it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1727.53it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1764.70it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1703.68it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1789.14it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1688.19it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1789.82it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1728.79it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1690.18it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1777.82it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1797.58it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1734.08it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1723.93it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1709.88it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1752.86it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1685.88it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1701.17it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1731.85it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1765.58it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1700.47it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1732.37it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1776.52it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1707.52it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1766.43it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1639.83it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1770.43it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1703.71it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 1621.59it/s]\n"
     ]
    }
   ],
   "source": [
    "mined_rules = []\n",
    "kb_extensions = []\n",
    "for model in models:\n",
    "    for method in entity_selection_methods:\n",
    "        for criteria in candidate_admittance_criteria:\n",
    "            extended_kb, admitted_candidates = extend_kb(original_kb, model, method, criteria)\n",
    "            rules = rule_mining(extended_kb)\n",
    "            kb_extensions.append(admitted_candidates)\n",
    "            mined_rules.append(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert metrics to correct datatype\n",
    "for rule_set in mined_rules:\n",
    "    rule_set['PCA Confidence'] = rule_set['PCA Confidence'].apply(lambda x: float(x.replace(',','.')))\n",
    "    rule_set['Head Coverage'] = rule_set['Head Coverage'].apply(lambda x: float(x.replace(',','.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rule                    object\n",
       "Head Coverage          float64\n",
       "PCA Confidence         float64\n",
       "Positive Examples       object\n",
       "PCA Body size           object\n",
       "Functional variable     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mined_rules[0].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mined rules to file\n",
    "with open(\"mined_rules.pkl\", \"wb\") as file:\n",
    "    pickle.dump(mined_rules, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_rules = pd.read_pickle(\"./mined_rules.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine to single dataframe\n",
    "Combine the list of rule set dataframes to a single large dataframe. Add columns for parameter values used to mine rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataframe that adds information about the parameters used to each row containing a rule\n",
    "if len(mined_rules) != len(parameter_combinations):\n",
    "    print(\"ERROR: number of given parameter combinaitons, \" + len(parameter_combinations) + \" is not equal to those actually used: \" + len(mined_rules))\n",
    "for i, parameter_row in parameter_combinations.iterrows():\n",
    "    number_of_rules = len(mined_rules[i])\n",
    "    parameter_list = parameter_row.values.tolist()\n",
    "    parameter_full = [copy.deepcopy(parameter_list) for j in range(number_of_rules)]\n",
    "    parameter_full_df = pd.DataFrame(parameter_full, columns=[\"Model\", \"Entity_selection\", \"Candidate_criteria\"])\n",
    "    mined_rules[i] = pd.concat([mined_rules[i], parameter_full_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add original rules to dataframe\n",
    "number_of_rules = len(original_rules)\n",
    "parameter_list = [\"Original rules\",\"Original rules\",\"Original rules\"]\n",
    "parameter_full = [parameter_list for j in range(number_of_rules)]\n",
    "parameter_full_df = pd.DataFrame(parameter_full, columns=[\"Model\", \"Entity_selection\", \"Candidate_criteria\"])\n",
    "original_rules_parameters= pd.concat([original_rules, parameter_full_df], axis=1)\n",
    "mined_rules.append(original_rules_parameters)\n",
    "mined_rules_parameters = pd.concat(mined_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine rule sets into one large dataframe\n",
    "mined_rules_parameters = pd.concat(mined_rules)\n",
    "\n",
    "# change datatype to string\n",
    "mined_rules_parameters['Candidate_criteria'] =  mined_rules_parameters.Candidate_criteria.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to file\n",
    "with open(\"mined_rules_parameters.pkl\", \"wb\") as file:\n",
    "    pickle.dump(mined_rules_parameters, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
